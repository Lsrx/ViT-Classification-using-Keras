{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edaac25",
   "metadata": {
    "id": "pigJ7NJQdpD1",
    "papermill": {
     "duration": 0.019497,
     "end_time": "2021-12-14T17:30:26.504547",
     "exception": false,
     "start_time": "2021-12-14T17:30:26.485050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implementação de uma ViT\n",
    "\n",
    "Author: Lucas Silva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0f307",
   "metadata": {
    "id": "wQCLGPTPd31M",
    "papermill": {
     "duration": 0.017942,
     "end_time": "2021-12-14T17:30:26.540808",
     "exception": false,
     "start_time": "2021-12-14T17:30:26.522866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introdução\n",
    "\n",
    "Usando o ViT: [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
    "model por Alexey Dosovitskiy et al. para classificação de ervas daninhas no cultivo da soja.\n",
    "\n",
    "Esse modelo ViT aplica a Arquitetura Transformer com auto-atenção para sequências de patches de imagens, **SEM** utilização de camadas de convolução.\n",
    "\n",
    "Será necessário o uso do TF Addons:\n",
    "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6979b",
   "metadata": {
    "id": "NRVZ0O7oekvl",
    "papermill": {
     "duration": 0.018117,
     "end_time": "2021-12-14T17:30:26.577008",
     "exception": false,
     "start_time": "2021-12-14T17:30:26.558891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5deb7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:26.626016Z",
     "iopub.status.busy": "2021-12-14T17:30:26.625304Z",
     "iopub.status.idle": "2021-12-14T17:30:31.345906Z",
     "shell.execute_reply": "2021-12-14T17:30:31.345129Z",
     "shell.execute_reply.started": "2021-12-14T17:20:54.529549Z"
    },
    "id": "57eT55D9emkC",
    "papermill": {
     "duration": 4.750927,
     "end_time": "2021-12-14T17:30:31.346067",
     "exception": false,
     "start_time": "2021-12-14T17:30:26.595140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import os, time, random, sys\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "import csv\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e1336",
   "metadata": {
    "id": "8OVOLqTlermz",
    "papermill": {
     "duration": 0.018357,
     "end_time": "2021-12-14T17:30:31.383030",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.364673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedb42de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:31.424491Z",
     "iopub.status.busy": "2021-12-14T17:30:31.423564Z",
     "iopub.status.idle": "2021-12-14T17:30:31.426609Z",
     "shell.execute_reply": "2021-12-14T17:30:31.426108Z",
     "shell.execute_reply.started": "2021-12-14T17:20:56.439432Z"
    },
    "papermill": {
     "duration": 0.025508,
     "end_time": "2021-12-14T17:30:31.426713",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.401205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runSeed():\n",
    "    global seed\n",
    "    seed=12\n",
    "    os.environ['PYTHONHASHSEED']=str(12)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "runSeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a7ac9",
   "metadata": {
    "id": "J7s2LKJ8e0MT",
    "papermill": {
     "duration": 0.017869,
     "end_time": "2021-12-14T17:30:31.462720",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.444851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurando os HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497d88e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:31.504879Z",
     "iopub.status.busy": "2021-12-14T17:30:31.503611Z",
     "iopub.status.idle": "2021-12-14T17:30:31.505861Z",
     "shell.execute_reply": "2021-12-14T17:30:31.506253Z",
     "shell.execute_reply.started": "2021-12-14T17:20:56.447714Z"
    },
    "papermill": {
     "duration": 0.025196,
     "end_time": "2021-12-14T17:30:31.506369",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.481173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "NUM_EPOCHS = 15\n",
    "MAX_EPOCH = 20\n",
    "RAW_IMG_SIZE = (256, 256)\n",
    "IMG_SIZE = (224, 224)\n",
    "INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "BATCH_SIZE = 32\n",
    "FOLDS = 5\n",
    "STOPPING_PATIENCE = 32\n",
    "LR_PATIENCE = 16\n",
    "INITIAL_LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8763dd65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:31.547694Z",
     "iopub.status.busy": "2021-12-14T17:30:31.546994Z",
     "iopub.status.idle": "2021-12-14T17:30:31.549755Z",
     "shell.execute_reply": "2021-12-14T17:30:31.549344Z",
     "shell.execute_reply.started": "2021-12-14T17:20:56.459095Z"
    },
    "id": "iF63VGQke3My",
    "papermill": {
     "duration": 0.025213,
     "end_time": "2021-12-14T17:30:31.549856",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.524643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#------------- Parâmetros ViT-Base -------------------------\n",
    "patch_size = 16  # Tamanho dos patches para serem extraidos.\n",
    "num_patches = (224 // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 12\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  \n",
    "\n",
    "# Tamanho das camadas de transformação.\n",
    "transformer_layers = 12\n",
    "mlp_head_units = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86dbd7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:31.591137Z",
     "iopub.status.busy": "2021-12-14T17:30:31.589964Z",
     "iopub.status.idle": "2021-12-14T17:30:31.592737Z",
     "shell.execute_reply": "2021-12-14T17:30:31.592259Z",
     "shell.execute_reply.started": "2021-12-14T17:20:56.469518Z"
    },
    "papermill": {
     "duration": 0.024619,
     "end_time": "2021-12-14T17:30:31.592840",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.568221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_DIRECTORY = '/kaggle/input/deepweeds/images/'\n",
    "LABEL_DIRECTORY = '/kaggle/input/deepweeds/labels/'\n",
    "OUTPUT_DIRECTORY = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45c5cd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:31.633604Z",
     "iopub.status.busy": "2021-12-14T17:30:31.632948Z",
     "iopub.status.idle": "2021-12-14T17:30:31.635658Z",
     "shell.execute_reply": "2021-12-14T17:30:31.635225Z",
     "shell.execute_reply.started": "2021-12-14T17:20:56.481299Z"
    },
    "papermill": {
     "duration": 0.024807,
     "end_time": "2021-12-14T17:30:31.635764",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.610957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSES_STR = ['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "CLASS_NAMES = ['Chinee Apple',\n",
    "               'Lantana',\n",
    "               'Parkinsonia',\n",
    "               'Parthenium',\n",
    "               'Prickly Acacia',\n",
    "               'Rubber Vine',\n",
    "               'Siam Weed',\n",
    "               'Snake Weed',\n",
    "               'Negatives']\n",
    "NUM_CLASSES=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b54e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:31.678439Z",
     "iopub.status.busy": "2021-12-14T17:30:31.677781Z",
     "iopub.status.idle": "2021-12-14T17:30:31.680530Z",
     "shell.execute_reply": "2021-12-14T17:30:31.680108Z",
     "shell.execute_reply.started": "2021-12-14T17:20:56.490621Z"
    },
    "papermill": {
     "duration": 0.026825,
     "end_time": "2021-12-14T17:30:31.680644",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.653819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop(img, size):\n",
    "    (h, w, c) = img.shape\n",
    "    x = int((w - size[0]) / 2)\n",
    "    y = int((h - size[1]) / 2)\n",
    "    return img[y:(y + size[1]), x:(x + size[0]), :]\n",
    "\n",
    "def crop_generator(batches, size):\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        (b, h, w, c) = batch_x.shape\n",
    "        batch_crops = np.zeros((b, size[0], size[1], c))\n",
    "        for i in range(b):\n",
    "            batch_crops[i] = crop(batch_x[i], (size[0], size[1]))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0e837",
   "metadata": {
    "papermill": {
     "duration": 0.017865,
     "end_time": "2021-12-14T17:30:31.716518",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.698653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setando os Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daaa370f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:30:31.764602Z",
     "iopub.status.busy": "2021-12-14T17:30:31.763899Z",
     "iopub.status.idle": "2021-12-14T17:31:23.999139Z",
     "shell.execute_reply": "2021-12-14T17:31:23.999726Z",
     "shell.execute_reply.started": "2021-12-14T17:20:56.503629Z"
    },
    "papermill": {
     "duration": 52.265296,
     "end_time": "2021-12-14T17:31:23.999951",
     "exception": false,
     "start_time": "2021-12-14T17:30:31.734655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10501 validated image filenames belonging to 9 classes.\n",
      "Found 3501 validated image filenames belonging to 9 classes.\n",
      "Found 3507 validated image filenames belonging to 9 classes.\n",
      "Found 10504 validated image filenames belonging to 9 classes.\n",
      "Found 3502 validated image filenames belonging to 9 classes.\n",
      "Found 3503 validated image filenames belonging to 9 classes.\n",
      "Found 10506 validated image filenames belonging to 9 classes.\n",
      "Found 3502 validated image filenames belonging to 9 classes.\n",
      "Found 3501 validated image filenames belonging to 9 classes.\n",
      "Found 10506 validated image filenames belonging to 9 classes.\n",
      "Found 3503 validated image filenames belonging to 9 classes.\n",
      "Found 3500 validated image filenames belonging to 9 classes.\n",
      "Found 10508 validated image filenames belonging to 9 classes.\n",
      "Found 3503 validated image filenames belonging to 9 classes.\n",
      "Found 3498 validated image filenames belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "for k in range(FOLDS):\n",
    "        # Prepare training, validation and testing labels for kth fold\n",
    "        train_label_file = \"{}train_subset{}.csv\".format(LABEL_DIRECTORY, k)\n",
    "        val_label_file = \"{}val_subset{}.csv\".format(LABEL_DIRECTORY, k)\n",
    "        test_label_file = \"{}test_subset{}.csv\".format(LABEL_DIRECTORY, k)\n",
    "        train_dataframe = pd.read_csv(train_label_file)\n",
    "        val_dataframe = pd.read_csv(val_label_file)\n",
    "        test_dataframe = pd.read_csv(test_label_file)\n",
    "        train_image_count = train_dataframe.shape[0]\n",
    "        val_image_count = train_dataframe.shape[0]\n",
    "        test_image_count = test_dataframe.shape[0]\n",
    "        train_dataframe['Label'] = train_dataframe.Label.astype(str)\n",
    "        val_dataframe['Label'] = val_dataframe.Label.astype(str)\n",
    "        test_dataframe['Label'] = test_dataframe.Label.astype(str)\n",
    "\n",
    "        # Training image augmentation\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale=1. / 255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # Validation image augmentation\n",
    "        val_data_generator = ImageDataGenerator(\n",
    "            rescale=1. / 255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # No testing image augmentation (except for converting pixel values to floats)\n",
    "        test_data_generator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "        # Load train images in batches from directory and apply augmentations\n",
    "        train_data_generator = train_data_generator.flow_from_dataframe(\n",
    "            train_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col='Filename',\n",
    "            y_col='Label',\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            #classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        # Load validation images in batches from directory and apply rescaling\n",
    "        val_data_generator = val_data_generator.flow_from_dataframe(\n",
    "            val_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            #classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        # Load test images in batches from directory and apply rescaling\n",
    "        test_data_generator = test_data_generator.flow_from_dataframe(\n",
    "            test_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            #classes=CLASSES,\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        # Crop augmented images from 256x256 to 224x224\n",
    "        train_data_generator = crop_generator(train_data_generator, IMG_SIZE)\n",
    "        val_data_generator = crop_generator(val_data_generator, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710241aa",
   "metadata": {
    "id": "nvyZYWZJfbdT",
    "papermill": {
     "duration": 0.0331,
     "end_time": "2021-12-14T17:31:24.068049",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.034949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implementando a Percepção de Multi Camada - Multi Layer Perception (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626582c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:31:24.140045Z",
     "iopub.status.busy": "2021-12-14T17:31:24.138642Z",
     "iopub.status.idle": "2021-12-14T17:31:24.144344Z",
     "shell.execute_reply": "2021-12-14T17:31:24.143772Z",
     "shell.execute_reply.started": "2021-12-14T17:21:03.293491Z"
    },
    "id": "QfZzNSJPfZyb",
    "papermill": {
     "duration": 0.044391,
     "end_time": "2021-12-14T17:31:24.144519",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.100128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45f3aa",
   "metadata": {
    "id": "fatiTu3jfltJ",
    "papermill": {
     "duration": 0.039764,
     "end_time": "2021-12-14T17:31:24.228356",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.188592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Declarando a criação de patches como uma camada da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6581e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:31:24.317159Z",
     "iopub.status.busy": "2021-12-14T17:31:24.316007Z",
     "iopub.status.idle": "2021-12-14T17:31:24.319265Z",
     "shell.execute_reply": "2021-12-14T17:31:24.320101Z",
     "shell.execute_reply.started": "2021-12-14T17:21:03.300666Z"
    },
    "id": "NeoxpS8efkPp",
    "papermill": {
     "duration": 0.052532,
     "end_time": "2021-12-14T17:31:24.320346",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.267814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414596e7",
   "metadata": {
    "id": "hBelAoc-gWIj",
    "papermill": {
     "duration": 0.039151,
     "end_time": "2021-12-14T17:31:24.399012",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.359861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implementando a camada de Patch Encoding\n",
    "\n",
    "A camada de `PatchEncoder` irá realizar a transformação linear de um patch, fazendo a projeção em um vetor de tamanho `projection_dim`. Junto, irá realizar a adição da posição de embedding para o vetor projetado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70a2b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:31:24.489667Z",
     "iopub.status.busy": "2021-12-14T17:31:24.488178Z",
     "iopub.status.idle": "2021-12-14T17:31:24.491252Z",
     "shell.execute_reply": "2021-12-14T17:31:24.490535Z",
     "shell.execute_reply.started": "2021-12-14T17:21:03.311161Z"
    },
    "id": "wTWINFfNg5_8",
    "papermill": {
     "duration": 0.052307,
     "end_time": "2021-12-14T17:31:24.491459",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.439152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152158b",
   "metadata": {
    "id": "05ahrMyCg8yr",
    "papermill": {
     "duration": 0.039818,
     "end_time": "2021-12-14T17:31:24.570924",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.531106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Construindo o modelo do ViT\n",
    "\n",
    "O modelo ViT consiste de múltiplos blocos Transformer, onde usamos o `layer.MultiHeadAttetion` como camada de self-attention, aplicando em uma sequência de patches. Os blocos Transformers produzem um tensor: `[batch_size,num_patches,projection_dim]`, que será processado via uma cabeça classificadora, com um softmax para produzir as probabilidades de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e052138d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:31:24.663146Z",
     "iopub.status.busy": "2021-12-14T17:31:24.660142Z",
     "iopub.status.idle": "2021-12-14T17:31:24.667334Z",
     "shell.execute_reply": "2021-12-14T17:31:24.668432Z",
     "shell.execute_reply.started": "2021-12-14T17:21:03.321254Z"
    },
    "id": "OPk0j3cph-zo",
    "papermill": {
     "duration": 0.057356,
     "end_time": "2021-12-14T17:31:24.668604",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.611248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
    "    # Criacao de patches\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode  dos patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Camadas do block transformer (range é limite de camadas)\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Camada MLP\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Cria um Tensor [batch_size, projection_dim].\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Adiciona MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classifica.\n",
    "    logits = layers.Dense(NUM_CLASSES,activation=\"sigmoid\")(features)\n",
    "    # Cria o modelo Keras.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdbbf8",
   "metadata": {
    "id": "Hev7sQb5iAqo",
    "papermill": {
     "duration": 0.040451,
     "end_time": "2021-12-14T17:31:24.748369",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.707918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compilando, treinando e avaliando o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28021579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:31:24.988134Z",
     "iopub.status.busy": "2021-12-14T17:31:24.987281Z",
     "iopub.status.idle": "2021-12-14T17:31:24.990455Z",
     "shell.execute_reply": "2021-12-14T17:31:24.989671Z",
     "shell.execute_reply.started": "2021-12-14T17:21:03.334925Z"
    },
    "papermill": {
     "duration": 0.205918,
     "end_time": "2021-12-14T17:31:24.990585",
     "exception": false,
     "start_time": "2021-12-14T17:31:24.784667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(OUTPUT_DIRECTORY + \"lastbest-0.hdf5\",save_weights_only=True, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(patience=STOPPING_PATIENCE, restore_best_weights=True)\n",
    "tensorboard = TensorBoard(log_dir=OUTPUT_DIRECTORY, histogram_freq=0, write_graph=True, write_images=False)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=LR_PATIENCE, min_lr=0.000003125)\n",
    "csv_logger = CSVLogger(OUTPUT_DIRECTORY + \"training_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12079d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T17:31:25.040424Z",
     "iopub.status.busy": "2021-12-14T17:31:25.039731Z",
     "iopub.status.idle": "2021-12-14T19:52:43.609771Z",
     "shell.execute_reply": "2021-12-14T19:52:43.609063Z",
     "shell.execute_reply.started": "2021-12-14T17:21:03.489020Z"
    },
    "id": "a5uYd9BWiOPX",
    "outputId": "55d4cba3-fcf3-44b9-cb0a-2f1352fb76ab",
    "papermill": {
     "duration": 8478.598525,
     "end_time": "2021-12-14T19:52:43.609927",
     "exception": false,
     "start_time": "2021-12-14T17:31:25.011402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "328/328 [==============================] - 512s 2s/step - loss: 0.4104 - categorical_accuracy: 0.4178 - val_loss: 0.2775 - val_categorical_accuracy: 0.5199\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27752, saving model to ./lastbest-0.hdf5\n",
      "Epoch 2/20\n",
      "328/328 [==============================] - 416s 1s/step - loss: 0.2973 - categorical_accuracy: 0.5059 - val_loss: 0.2782 - val_categorical_accuracy: 0.5202\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.27752\n",
      "Epoch 3/20\n",
      "328/328 [==============================] - 417s 1s/step - loss: 0.2865 - categorical_accuracy: 0.5085 - val_loss: 0.2673 - val_categorical_accuracy: 0.5202\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.27752 to 0.26727, saving model to ./lastbest-0.hdf5\n",
      "Epoch 4/20\n",
      "328/328 [==============================] - 410s 1s/step - loss: 0.2746 - categorical_accuracy: 0.5182 - val_loss: 0.2685 - val_categorical_accuracy: 0.5180\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26727\n",
      "Epoch 5/20\n",
      "328/328 [==============================] - 412s 1s/step - loss: 0.2689 - categorical_accuracy: 0.5109 - val_loss: 0.2346 - val_categorical_accuracy: 0.5539\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26727 to 0.23462, saving model to ./lastbest-0.hdf5\n",
      "Epoch 6/20\n",
      "328/328 [==============================] - 420s 1s/step - loss: 0.2545 - categorical_accuracy: 0.5220 - val_loss: 0.2279 - val_categorical_accuracy: 0.5406\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23462 to 0.22788, saving model to ./lastbest-0.hdf5\n",
      "Epoch 7/20\n",
      "328/328 [==============================] - 419s 1s/step - loss: 0.2306 - categorical_accuracy: 0.5572 - val_loss: 0.2184 - val_categorical_accuracy: 0.5577\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22788 to 0.21836, saving model to ./lastbest-0.hdf5\n",
      "Epoch 8/20\n",
      "328/328 [==============================] - 416s 1s/step - loss: 0.2287 - categorical_accuracy: 0.5475 - val_loss: 0.2189 - val_categorical_accuracy: 0.5652\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21836\n",
      "Epoch 9/20\n",
      "328/328 [==============================] - 421s 1s/step - loss: 0.2196 - categorical_accuracy: 0.5645 - val_loss: 0.2144 - val_categorical_accuracy: 0.5676\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.21836 to 0.21436, saving model to ./lastbest-0.hdf5\n",
      "Epoch 10/20\n",
      "328/328 [==============================] - 418s 1s/step - loss: 0.2177 - categorical_accuracy: 0.5709 - val_loss: 0.2016 - val_categorical_accuracy: 0.5912\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21436 to 0.20160, saving model to ./lastbest-0.hdf5\n",
      "Epoch 11/20\n",
      "328/328 [==============================] - 415s 1s/step - loss: 0.2115 - categorical_accuracy: 0.5907 - val_loss: 0.2053 - val_categorical_accuracy: 0.5856\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20160\n",
      "Epoch 12/20\n",
      "328/328 [==============================] - 419s 1s/step - loss: 0.2117 - categorical_accuracy: 0.5756 - val_loss: 0.1901 - val_categorical_accuracy: 0.6263\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.20160 to 0.19011, saving model to ./lastbest-0.hdf5\n",
      "Epoch 13/20\n",
      "328/328 [==============================] - 417s 1s/step - loss: 0.2022 - categorical_accuracy: 0.6026 - val_loss: 0.1939 - val_categorical_accuracy: 0.6117\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19011\n",
      "Epoch 14/20\n",
      "328/328 [==============================] - 421s 1s/step - loss: 0.2024 - categorical_accuracy: 0.6009 - val_loss: 0.1922 - val_categorical_accuracy: 0.6118\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19011\n",
      "Epoch 15/20\n",
      "328/328 [==============================] - 421s 1s/step - loss: 0.1956 - categorical_accuracy: 0.6141 - val_loss: 0.1833 - val_categorical_accuracy: 0.6261\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.19011 to 0.18332, saving model to ./lastbest-0.hdf5\n",
      "Epoch 16/20\n",
      "328/328 [==============================] - 422s 1s/step - loss: 0.1910 - categorical_accuracy: 0.6198 - val_loss: 0.1855 - val_categorical_accuracy: 0.6392\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18332\n",
      "Epoch 17/20\n",
      "328/328 [==============================] - 416s 1s/step - loss: 0.1911 - categorical_accuracy: 0.6171 - val_loss: 0.1801 - val_categorical_accuracy: 0.6441\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.18332 to 0.18012, saving model to ./lastbest-0.hdf5\n",
      "Epoch 18/20\n",
      "328/328 [==============================] - 422s 1s/step - loss: 0.1842 - categorical_accuracy: 0.6385 - val_loss: 0.1827 - val_categorical_accuracy: 0.6418\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18012\n",
      "Epoch 19/20\n",
      "328/328 [==============================] - 426s 1s/step - loss: 0.1861 - categorical_accuracy: 0.6319 - val_loss: 0.1738 - val_categorical_accuracy: 0.6520\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.18012 to 0.17381, saving model to ./lastbest-0.hdf5\n",
      "Epoch 20/20\n",
      "328/328 [==============================] - 422s 1s/step - loss: 0.1813 - categorical_accuracy: 0.6401 - val_loss: 0.1756 - val_categorical_accuracy: 0.6515\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.17381\n",
      "Completed training after 20 epochs.\n"
     ]
    }
   ],
   "source": [
    "model = create_vit_classifier()\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=INITIAL_LR), metrics=['categorical_accuracy'])\n",
    "\n",
    "global_epoch = 0\n",
    "restarts = 0\n",
    "last_best_losses = []\n",
    "last_best_epochs = []\n",
    "while global_epoch < MAX_EPOCH:\n",
    "    history = model.fit(\n",
    "        train_data_generator,\n",
    "        steps_per_epoch=train_image_count // BATCH_SIZE,\n",
    "        epochs=MAX_EPOCH - global_epoch, #alterar depois\n",
    "        validation_data=val_data_generator,\n",
    "        validation_steps=val_image_count // BATCH_SIZE,\n",
    "        callbacks=[tensorboard, model_checkpoint, early_stopping, reduce_lr, csv_logger],\n",
    "        shuffle=False)\n",
    "    if early_stopping.stopped_epoch == 0:\n",
    "        print(\"Completed training after {} epochs.\".format(MAX_EPOCH))\n",
    "        break\n",
    "    else:\n",
    "        global_epoch = global_epoch + early_stopping.stopped_epoch - STOPPING_PATIENCE + 1\n",
    "        print(\"Early stopping triggered after local epoch {} (global epoch {}).\".format(\n",
    "            early_stopping.stopped_epoch, global_epoch))\n",
    "        print(\"Restarting from last best val_loss at local epoch {} (global epoch {}).\".format(\n",
    "            early_stopping.stopped_epoch - STOPPING_PATIENCE, global_epoch - STOPPING_PATIENCE))\n",
    "        restarts = restarts + 1\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=INITIAL_LR / 2 ** restarts),\n",
    "                      metrics=['categorical_accuracy'])\n",
    "        model_checkpoint = ModelCheckpoint(OUTPUT_DIRECTORY + \"lastbest-{}.hdf5\".format(restarts),\n",
    "                                           monitor='val_loss',save_weights_only=True, verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Save last best model info\n",
    "# with open(OUTPUT_DIRECTORY + \"last_best_models.csv\", 'w', newline='') as file:\n",
    "#     writer = csv.writer(file, delimiter=',')\n",
    "#     writer.writerow(['Model file', 'Global epoch', 'Validation loss'])\n",
    "#     for i in range(restarts + 1):\n",
    "#         writer.writerow([\"lastbest-{}.hdf5\".format(i), last_best_epochs[i], last_best_losses[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28015e",
   "metadata": {
    "papermill": {
     "duration": 1.998804,
     "end_time": "2021-12-14T19:52:47.460127",
     "exception": false,
     "start_time": "2021-12-14T19:52:45.461323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Trabalhando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5c343d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T19:52:50.960091Z",
     "iopub.status.busy": "2021-12-14T19:52:50.955009Z",
     "iopub.status.idle": "2021-12-14T19:52:51.803134Z",
     "shell.execute_reply": "2021-12-14T19:52:51.802550Z",
     "shell.execute_reply.started": "2021-12-14T17:28:32.186973Z"
    },
    "papermill": {
     "duration": 2.591441,
     "end_time": "2021-12-14T19:52:51.803272",
     "exception": false,
     "start_time": "2021-12-14T19:52:49.211831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/ViT_base_weights')\n",
    "model_weight_save = ModelCheckpoint(OUTPUT_DIRECTORY + \"model_best_weights.hdf5\",save_weights_only=True, verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936a1f2",
   "metadata": {
    "papermill": {
     "duration": 1.805026,
     "end_time": "2021-12-14T19:52:55.363537",
     "exception": false,
     "start_time": "2021-12-14T19:52:53.558511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metricas por Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "194df478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T19:52:59.189489Z",
     "iopub.status.busy": "2021-12-14T19:52:59.188733Z",
     "iopub.status.idle": "2021-12-14T19:52:59.719849Z",
     "shell.execute_reply": "2021-12-14T19:52:59.719330Z",
     "shell.execute_reply.started": "2021-12-14T17:28:33.300098Z"
    },
    "papermill": {
     "duration": 2.531137,
     "end_time": "2021-12-14T19:52:59.719990",
     "exception": false,
     "start_time": "2021-12-14T19:52:57.188853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "# model_load = create_vit_classifier()\n",
    "# model_load.load_weights('./checkpoints/ViT_base_weights')\n",
    "\n",
    "# # Evaluate model on test subset for kth fold\n",
    "# predictions = model_load.predict(test_data_generator, test_image_count // BATCH_SIZE + 1)\n",
    "# y_true = test_data_generator.classes\n",
    "# y_pred = np.argmax(predictions, axis=1)\n",
    "# y_pred[np.max(predictions, axis=1) < 1 / 9] = 8  # Assign predictions worse than random guess to negative class\n",
    "\n",
    "# Evaluate model on test subset for kth fold\n",
    "# predictions = model.predict(test_data_generator, test_image_count // BATCH_SIZE + 1)\n",
    "# y_true = test_data_generator.classes\n",
    "# y_pred = np.argmax(predictions, axis=1)\n",
    "# y_pred[np.max(predictions, axis=1) < 1 / 9] = 8  # Assign predictions worse than random guess to negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b205b200",
   "metadata": {
    "papermill": {
     "duration": 1.714634,
     "end_time": "2021-12-14T19:53:03.205416",
     "exception": false,
     "start_time": "2021-12-14T19:53:01.490782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Métricas por Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c023f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T19:53:06.716708Z",
     "iopub.status.busy": "2021-12-14T19:53:06.715468Z",
     "iopub.status.idle": "2021-12-14T19:53:47.324432Z",
     "shell.execute_reply": "2021-12-14T19:53:47.323973Z",
     "shell.execute_reply.started": "2021-12-14T17:29:15.478631Z"
    },
    "papermill": {
     "duration": 42.352809,
     "end_time": "2021-12-14T19:53:47.324554",
     "exception": false,
     "start_time": "2021-12-14T19:53:04.971745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "  Chinee Apple       0.33      0.05      0.09       225\n",
      "       Lantana       0.56      0.46      0.50       212\n",
      "   Parkinsonia       0.37      0.72      0.49       206\n",
      "    Parthenium       0.34      0.09      0.15       204\n",
      "Prickly Acacia       0.55      0.79      0.65       212\n",
      "   Rubber Vine       0.73      0.18      0.29       201\n",
      "     Siam Weed       0.53      0.58      0.55       214\n",
      "    Snake Weed       0.36      0.09      0.15       203\n",
      "     Negatives       0.70      0.84      0.76      1821\n",
      "\n",
      "      accuracy                           0.61      3498\n",
      "     macro avg       0.50      0.42      0.40      3498\n",
      "  weighted avg       0.59      0.61      0.57      3498\n",
      "\n",
      "[[  12    9   18    1    9    0    7   29  140]\n",
      " [   4   97    1    1    0    2   33    2   72]\n",
      " [   0    0  149    0   21    0    0    0   36]\n",
      " [   0    3   45   19   39    0    1    0   97]\n",
      " [   0    0   36    0  168    0    0    0    8]\n",
      " [   8   11    6    0   20   37   22    0   97]\n",
      " [   0   15    0    0    0    1  124    0   74]\n",
      " [   7    9   17    3   11    0    3   19  134]\n",
      " [   5   29  131   32   40   11   44    3 1526]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.05333333, 0.45754717, 0.72330097, 0.09313725, 0.79245283,\n",
       "       0.1840796 , 0.57943925, 0.09359606, 0.8380011 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_generator(test_data_generator, test_image_count // BATCH_SIZE + 1)\n",
    "y_true = test_data_generator.classes\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_pred[np.max(predictions, axis=1) < 1 / 9] = 8\n",
    "\n",
    "# Generate and print classification metrics and confusion matrix\n",
    "print(classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES))\n",
    "report = classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES, output_dict=True)\n",
    "with open('classification_report.csv', 'w') as f:\n",
    "    for key in report.keys():\n",
    "        f.write(\"%s,%s\\n\" % (key, report[key]))\n",
    "conf_arr = confusion_matrix(y_true, y_pred, labels=CLASSES)\n",
    "print(conf_arr)\n",
    "\n",
    "#Get the confusion matrix\n",
    "cm = conf_arr\n",
    "\n",
    "#Now the normalize the diagonal entries\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "#The diagonal entries are the accuracies of each class\n",
    "cm.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e10b2cdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T19:53:50.797223Z",
     "iopub.status.busy": "2021-12-14T19:53:50.796339Z",
     "iopub.status.idle": "2021-12-14T19:53:50.801892Z",
     "shell.execute_reply": "2021-12-14T19:53:50.801350Z",
     "shell.execute_reply.started": "2021-12-14T17:29:34.749784Z"
    },
    "papermill": {
     "duration": 1.747504,
     "end_time": "2021-12-14T19:53:50.802012",
     "exception": false,
     "start_time": "2021-12-14T19:53:49.054508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 8 8 ... 8 2 8]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8615.64773,
   "end_time": "2021-12-14T19:53:55.438509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-14T17:30:19.790779",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
